{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdbf7859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando script...\n",
      "Carregando dataset principal...\n",
      "Criando matriz de conexão...\n",
      "   user  item  rate\n",
      "0   262   303     1\n",
      "1   322   433     1\n",
      "2   323   433     1\n",
      "3   237   433     1\n",
      "4   260   482     1\n",
      "Reduzindo dimensionalidade\n",
      "Tamanho inicial ---  244869\n",
      "Redução completa\n",
      "Usuários restantes ---  999\n",
      "Items restantes ---  2562\n",
      "Tamanho final da base ---  4485\n",
      "Usuário e documentos finais: 459 9\n",
      "Adicionando itens não explicitos a matriz...\n",
      "Processamento completo. Criando dataframe com  0  items.\n",
      "Limpando dados...\n",
      "----------- Starting training -----------\n",
      "Parameters -----------\n",
      "MFSIZE ----------- 2\n",
      "UW ----------- 0.1\n",
      "IW ----------- 0.1\n",
      "LR ----------- 0.001\n",
      "BATCH_SIZE ----------- 1000\n",
      "EPOCH_MAX ----------- 100\n",
      "(1000, 1000)\n",
      "(6000, 6000)\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:195: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:64: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:206: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:95: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:99: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:209: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:210: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\joaov\\graphrec-script\\utils\\graphrec_automated.py:213: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "epoch train_error val_error elapsed_time\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4396b2348654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;31m### Get best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m \u001b[0mbest_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_ur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_ir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_mfsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;31m#### Primeira execução sem informações dos artigos:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-4396b2348654>\u001b[0m in \u001b[0;36mgrid_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m    163\u001b[0m                             \u001b[0mLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                             \u001b[0mEPOCH_MAX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                             BATCH_SIZE = 1000) \n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# retorna o dataset de treino ordenado com base nos ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\graphrec-script\\utils\\graphrec_automated.py\u001b[0m in \u001b[0;36mGraphRec\u001b[1;34m(train, test, ItemData, UserData, Graph, Dataset, USER_NUM, ITEM_NUM, MFSIZE, UW, IW, LR, EPOCH_MAX, BATCH_SIZE)\u001b[0m\n\u001b[0;32m    224\u001b[0m                                                                    \u001b[0mitem_batch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                                                                    \u001b[0mrate_batch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                                                                    phase:True})\n\u001b[0m\u001b[0;32m    227\u001b[0m             \u001b[0mpred_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_batch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-3-6\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from os.path import exists\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utils.graphrec_automated import GraphRec\n",
    "from utils.metrics import queries_ndcg, mean_ap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "POSITIVE_VALUE = 5\n",
    "NEGATIVE_VALUE = 1\n",
    "\n",
    "USER_NUM=1000\n",
    "ITEM_NUM=6000\n",
    "\n",
    "def get_dataset(dataset, perc=0.9):\n",
    "    rows = len(dataset)\n",
    "    df = dataset.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "        df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "\n",
    "    split_index = int(rows * perc)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "\n",
    "    # test_p = len(df_test[df_test[\"rate\"]==POSITIVE_VALUE])\n",
    "    # test_n = len(df_test[df_test[\"rate\"]==NEGATIVE_VALUE])\n",
    "\n",
    "    # train_p = len(df_train[df_train[\"rate\"]==POSITIVE_VALUE])\n",
    "    # train_n = len(df_train[df_train[\"rate\"]==NEGATIVE_VALUE])\n",
    "\n",
    "    # print(\"ITENS POSITIVOS EM TESTE: \", test_p)\n",
    "    # print(\"ITENS POSITIVOS EM TREINO: \", train_p)\n",
    "\n",
    "    # print(\"ITENS NEGATIVOS EM TESTE: \", test_n)\n",
    "    # print(\"ITENS NEGATIVOS EM TREINO: \", train_n)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def reduce_dimensionality(df_ratings, MAX_USERS_DESIRED = USER_NUM, MAX_ITEMS_DESIRED = ITEM_NUM):\n",
    "    print(\"Reduzindo dimensionalidade\")\n",
    "    print(\"Tamanho inicial --- \", df_ratings.size)\n",
    "    index_max_user = df_ratings.loc[df_ratings.user==MAX_USERS_DESIRED].index[0]\n",
    "    df_ratings = df_ratings[:index_max_user]\n",
    "    df_ratings.drop(df_ratings[df_ratings['item'] >= MAX_ITEMS_DESIRED].index, inplace = True)\n",
    "    print(\"Redução completa\")\n",
    "    print(\"Usuários restantes --- \", df_ratings['user'].max())\n",
    "    print(\"Items restantes --- \", df_ratings['item'].max())\n",
    "    print(\"Tamanho final da base --- \", df_ratings.size)\n",
    "    return df_ratings\n",
    "\n",
    "  \n",
    "def read_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def pre_process_dataset():\n",
    "    print(\"Iniciando script...\")\n",
    "    print(\"Carregando dataset principal...\")\n",
    "    df_user_item = pd.read_csv('./data/dblp/relation_list.txt',  sep=\"\\t\")\n",
    "    ### Transformar essa lista em uma matriz de conexão\n",
    "    print(\"Criando matriz de conexão...\")\n",
    "\n",
    "    rows_list = []\n",
    "#     for ind in df_user_item.index:\n",
    "#         user = df_user_item.iloc[[ind]]\n",
    "#         user_ratings = list(map(int, df_user_item[0][ind].split()))\n",
    "#         for rating in user_ratings:\n",
    "#             user_rating = [user['id'], rating, 1]\n",
    "#             rows_list.append(user_rating)\n",
    "    df_user_item['rate'] = 1\n",
    "    df_user_item = df_user_item.rename({'article': 'item'}, axis=1)\n",
    "    df_ratings = df_user_item\n",
    "    print(df_ratings.head())\n",
    "\n",
    "    ### Redução de dimensionalidade, se desejado\n",
    "    df_ratings_sample = reduce_dimensionality(df_ratings, USER_NUM, ITEM_NUM)\n",
    "    del df_ratings\n",
    "\n",
    "    docs = df_ratings_sample['item'].unique()\n",
    "    users = df_ratings_sample['user'].unique()\n",
    "    rows_list = []\n",
    "    ##df_ratings_complete = df_ratings_complete.iloc[0:0]\n",
    "\n",
    "    print(\"Usuário e documentos finais:\", users.size, docs.size)\n",
    "    count = 0\n",
    "\n",
    "    print(\"Adicionando itens não explicitos a matriz...\")\n",
    "    ### Cria todas as conexões restantes não apresentadas no dataset inicial - Rate == 0\n",
    "    # NEGATIVE_RATIO = 50\n",
    "    # for ind in users:\n",
    "    #     count = 0\n",
    "    #     user_id = ind\n",
    "    #     user_ratings = list(map(int, df_ratings_sample.loc[df_ratings_sample['user'] == user_id, 'item']))\n",
    "    #     negative_docs = [doc for doc in docs if doc not in user_ratings]\n",
    "    #     # print(\"Process nr. \", count,\" ### User -- \", ind, \"with\", len(user_ratings), \"items\")\n",
    "    #     ### Adiciona todos os itens positivos a lista\n",
    "    #     for positive_doc in user_ratings:  \n",
    "    #         user_rating = [user_id, positive_doc, 1]\n",
    "    #         # print(\"Positive docs\", positive_doc)\n",
    "    #         rows_list.append(user_rating)\n",
    "    #         ### Para cada item positivo, pega uma quantidade negativa baseado no ratio 1:NEGATIVE RATIO\n",
    "    #         random.seed(count)\n",
    "    #         negative_list = random.sample(negative_docs, NEGATIVE_RATIO)\n",
    "    #         # print(\"Negative docs\", negative_list)\n",
    "    #         for negative_doc in negative_list:\n",
    "    #             negative_user_rating = [user_id, negative_doc, 0]\n",
    "    #             rows_list.append(negative_user_rating)\n",
    "    #         count = count+1\n",
    "\n",
    "    ### Dataset completo\n",
    "    print(\"Processamento completo. Criando dataframe com \", len(rows_list), \" items.\")\n",
    "    ### Substitui os valores de 0 e 1 para valores mais compreensiveis pelo graphrec\n",
    "    # df_ratings_complete = pd.DataFrame(columns=['user', 'item', 'rate'], data=rows_list)\n",
    "    df_ratings_complete = df_ratings_sample\n",
    "    df_ratings_complete.loc[df_ratings_complete['rate'] == 1, 'rate'] = POSITIVE_VALUE\n",
    "    # df_ratings_complete.loc[df_ratings_complete['rate'] == 0, 'rate'] = NEGATIVE_VALUE\n",
    "    # random_sampling = df_ratings_complete.groupby(\"rate\").sample(n=14000, random_state=42)\n",
    "    print(\"Limpando dados...\")\n",
    "    del rows_list\n",
    "    del docs\n",
    "    del users\n",
    "    del df_ratings_sample\n",
    "    return df_ratings_complete\n",
    "\n",
    "def grid_search():\n",
    "    learning_rates = [0.001, 0.00005, 0.000005]\n",
    "    user_regularizers = [0.1, 0.05, 0.01]\n",
    "    item_regularizers = [0.1, 0.05,  0.01]\n",
    "    mf_sizes = [2, 30, 100]\n",
    "\n",
    "    best_rmse = 0\n",
    "\n",
    "    best_lr = 0\n",
    "    best_ur = 0\n",
    "    best_ir = 0\n",
    "    best_mfsize = 0\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for ur in user_regularizers:\n",
    "            for ir in item_regularizers:\n",
    "                for mf_size in mf_sizes:\n",
    "                    df_train, df_test = get_dataset(df_ratings_complete)\n",
    "\n",
    "                    model = GraphRec(df_train, df_test, ItemData=False, UserData = False, Graph=False, Dataset='DBLP', USER_NUM=USER_NUM, ITEM_NUM=ITEM_NUM, \n",
    "                            MFSIZE=mf_size,\n",
    "                            UW=ur,\n",
    "                            IW=ir,\n",
    "                            LR=lr,\n",
    "                            EPOCH_MAX = 100,\n",
    "                            BATCH_SIZE = 1000) \n",
    "                    \n",
    "                    df_test = df_test.sort_values('user') # retorna o dataset de treino ordenado com base nos ids\n",
    "                    y_test = df_test['rate']                    \n",
    "                    predictions = np.array(model.predict(df_test)).flatten() # model.predict(df_test)\n",
    "                    rmse = mean_squared_error(y_test, predictions, squared = False) # retorna a raiz quadradica do erro-medio\n",
    "\n",
    "                    if(rmse > best_rmse):\n",
    "                        best_lr = lr\n",
    "                        best_ur = ur\n",
    "                        best_ir = ir\n",
    "                        best_mfsize = mf_size\n",
    "\n",
    "                        print(\"Parâmetros melhores encontrados: \")\n",
    "                        print(\"LR --- \", best_lr)\n",
    "                        print(\"UR --- \", best_ur)\n",
    "                        print(\"IR --- \", best_ir)\n",
    "                        print(\"MF_SIZE --- \", best_mfsize)\n",
    "                        print(\"RMSE --- \", rmse)\n",
    "                        best_rmse = rmse\n",
    "                    else:\n",
    "                        print(\"RMSE inferior ao melhor ---\", rmse)\n",
    "    return  best_lr, best_ur, best_ir, best_mfsize\n",
    "    \n",
    "### Check if pre processed dataset is already saved\n",
    "preprocessed_path = 'preprocess/dblp_pre_processed_dataset-1000:5000.csv'\n",
    "file_exists = exists(preprocessed_path)\n",
    "\n",
    "if(not file_exists):\n",
    "    df_ratings_complete = pre_process_dataset()\n",
    "    df_ratings_complete.to_csv(preprocessed_path)\n",
    "else:\n",
    "    df_ratings_complete = pd.read_csv(preprocessed_path)\n",
    "\n",
    "### Get best parameters\n",
    "best_lr, best_ur, best_ir, best_mfsize = grid_search()\n",
    "\n",
    "#### Primeira execução sem informações dos artigos:\n",
    "df_train, df_test = get_dataset(df_ratings_complete)\n",
    "print(\"------------- Execução com os melhores parâmetros: Sem informações adicionais dos artigos: ---------------\")\n",
    "model = GraphRec(df_train, df_test, ItemData=False, UserData = False, Graph=False, Dataset='DBLP', USER_NUM=USER_NUM, ITEM_NUM=ITEM_NUM, \n",
    "            MFSIZE=best_mfsize,\n",
    "            UW=best_ur,\n",
    "            IW=best_ir,\n",
    "            LR=best_lr,\n",
    "            EPOCH_MAX = 200,\n",
    "            BATCH_SIZE = 1000) \n",
    "print(\"Execução da predição para teste\")\n",
    "df_test = df_test.sort_values('user') # retorna o dataset de treino ordenado com base nos ids\n",
    "qids = df_test['user'] # qids sao os ids dos usuarios\n",
    "y_test = df_test['rate'] # y_test sao os scores verdadeiros do teste\n",
    "predictions = np.array(model.predict(df_test)).flatten() # model.predict(df_test)\n",
    "\n",
    "result_df = df_test.copy(deep=True)\n",
    "result_df = result_df.assign(prediction=predictions)\n",
    "\n",
    "print(\"Resultados Primeira Execução: \")\n",
    "ndcgs = queries_ndcg(y_test, predictions, qids) # retorna uma lista com ndcg de cada query (que seria id de cada usuario)\n",
    "print(\"MEAN NDCGS:\", ndcgs.mean())\n",
    "rmse = mean_squared_error(y_test, predictions, squared = False) # retorna a raiz quadradica do erro-medio\n",
    "print(\"RMSE:\", rmse)\n",
    "map_res = mean_ap(result_df, 5.0)\n",
    "print(\"MAP:\", map_res)\n",
    "print(\"------------- Execução finalizada ---------------\")\n",
    "\n",
    "\n",
    "#### Segunda execução: Com informações dos artigos:\n",
    "# print(\"------------- Segunda execução com os melhores parâmetros: Com informações adicionais dos artigos: ---------------\")\n",
    "# df_train, df_test = get_data_citeulike(df_ratings_complete)\n",
    "# model = GraphRec(df_train, df_test, ItemData=True, UserData = False, Graph=True, Dataset='citeU', USER_NUM=USER_NUM, ITEM_NUM=ITEM_NUM)\n",
    "# print(\"Execução da predição para teste\")\n",
    "# df_test = df_test.sort_values('user') # retorna o dataset de treino ordenado com base nos ids\n",
    "# qids = df_test['user'] # qids sao os ids dos usuarios\n",
    "# y_test = df_test['rate'] # y_test sao os scores verdadeiros do teste\n",
    "# predictions = np.array(model.predict(df_test)).flatten() # model.predict(df_test) \n",
    "\n",
    "# result_df = df_test.copy(deep=True)\n",
    "# result_df = result_df.assign(prediction=predictions)\n",
    "\n",
    "# print(\"Resultados Segunda Execução: \")\n",
    "# ndcgs = queries_ndcg(y_test, predictions, qids) # retorna uma lista com ndcg de cada query (que seria id de cada usuario)\n",
    "# print(\"MEAN NDCGS:\", ndcgs.mean())\n",
    "# rmse = mean_squared_error(y_test, predictions, squared = False) # retorna a raiz quadradica do erro-medio\n",
    "# print(\"RMSE:\", rmse)\n",
    "# map_res = mean_ap(result_df, 5.0)\n",
    "# print(\"MAP:\", map_res)\n",
    "# print(\"------------- Execução finalizada ---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f438a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
